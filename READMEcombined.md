(This README file is a concatenation of the main folder README and first-level READMEs. The final code repository will contain all the READMEs in their correct directories)

# CSCI-GA.3033-003 - Project: Finding Napa
## R. Feng, C. Gilbert, G. Ng
This repository contains the code, documentation, and summary data for the *Finding Napa* project. This repository is split into subdirectories by functional category. Each subfolder contains additional information in their associated README files. 

## Subdirectories

* `composite` contains the code used to join cosine similarity indices generated by the soil, solar, and weather data. This folder contains the final output similarity data.
* `soil` contains the scripts and data used to retrieve, profile, and process the USDA soil composition data.
* `Solar` contains the scripts and data used to retrieve, profile, and process the NRSDB solar irradiance data.
* `weather` contains the scripts and data used to retrieve, profile, and process the NOAA weather data.
* `tools` contains shared development tools

(composite README)
# Composite Score (Final Analytic)
This folder contains Hive queries used to combine soil, weather, and solar radiance cosine similarities into a composite score describing each soil area's similarity to Napa. Currently the composite score is a simple average of the three cosine similarities, with possible values ranging from -1 to +1. 

Note that one intermediate step is to wrangle weather similarities from long-format (each row is a station-decade/Napa-decade comparison) to wide-format (each row is a station), where we defined each weather region's similarity with Napa-2010 in each decade. This added step is necessary because unlike soil and solar, weather data had a time dimension. 

The resulting `weather_sim` contains three columns of weather similarities: `weather_sim_2000` describes how similar each station's 2000 weather is to today's Napa (2010), `weather_sim_2010` describes how similar each station's 2010 weather is to today's Napa, and `weather_sim_future` estimates how similar each station's future weather will be to today's Napa (assuming that the former goes up by 5 degrees by 2100).   

Another intermediate step here is to derive a mapping of region IDs across the three datasets: `lkey` for soil, `solar_region_key` for solar radiance, and `station_id` for weather. Mapping between `lkey` and `solar_region_key` is trivial, since we queried the solar radiance API by soil's lat/longs, essentially obtaining each `solar_region_key` from a known `lkey`. 

Mapping between `lkey` and weather `station_id` is more involved: we took a cross product between the soil region (`legend.txt`) and weather region (`weather_stations.txt`) meta tables, computed the haversine distance between each pair's latitudes and longitudes, then for each `lkey` select the `station_id` with the smallest distance (subject to a few other constraints) as its mapped `station_id`. While this involves a `CROSS JOIN` in Hive, the solution is highly tractable even with no additional optimization (query ran within a couple minutes) since there are only a few thousands of soil areas and weather areas.  

## Hive Query Files 
* `wrangle_weather_sim.sql` wrangles weather similarities from long- to wide-format 
* `map_regions` maps region IDs across the soil, weather, and solar datasets
* `compute_final_analytic` computes composite score by taking simple average of the three similarities 

## HDFS Input Data

Lat/longs and other meta data for each soil, weather, and solar region can be found in: 

| Table Content | HDFS File Path | Hive Table | 
| ----------- | ----------- | ----------- | 
| Soil Areas | /user/yjn214/rbda-proj/legend.txt | yjn214.db/legend | 
| Solar Regions | /user/yjn214/rbda-proj/solarRegionsV2.csv | yjn214.db/solar_regions | 
| Weather Stations | /user/yjn214/rbda-proj/weather_stations.txt | yjn214.db/weather_stations | 

Cosine similarities for soil, weather, and solar regions as computed from previous pipelines can be found in: 

| Table Content | HDFS File Path | Hive Table | 
| ----------- | ----------- | ----------- | 
| Soil Similarities | /user/yjn214/rbda-proj/soil_cos_sim | yjn214.db/soil_sim | 
| Solar Similarities | /user/cjg507/cosineSim | yjn214.db/solar_sim | 
| Weather Similarities | /user/rf1316/cos_sim | yjn214.db/weather_sim_1 | 
| Weather Similarities (with 5 deg offset) | /user/rf1316/cos_sim2 | yjn214.db/weather_sim_2 | 

## HDFS Output Data

Below are the relevant output tables in HDFS and Hive: 

| Table Content | HDFS File Path | Hive Table | 
| ----------- | ----------- | ----------- | 
| Region Mapping | hdfs://dumbo/user/hive/warehouse/yjn214.db/region_mapping_local | yjn214.db/region_mapping | 
| Composite Score | hdfs://dumbo/user/hive/warehouse/yjn214.db/composite_sim_local | yjn214.db/composite_sim | 

(soil README)
# Soil Data
This folder contains the code used to process soil attribute data obtained from USDA NRCS (https://sdmdataaccess.sc.egov.usda.gov/). The goal is to extract and aggregate features for each soil region, and compute their cosine similarity against Napa. The resulting soil similarity is one of three components in the composite soil similarity used in the final analytic for the *Finding Napa* project. 

## Folders
* `ingest_data` contains SQL queries used to extract input data from USDA database, and scripts to load them into HDFS
* `mr_process` contains MapReduce code used to profile and clean input data 
* `hive_featurize` contains Hive queries used to standardize and aggregate soil horizon/component-level attributes into soil area-level features 
* `mr_cos_sim` contains MapReduce code used to compute cosine similarities between each soil area and Napa 

## HDFS Input Data

Soil data is provided at four hierarchical levels: 
1. a soil area is composed of several map units 
2. a map unit is composed of several components 
3. a component is composed of several horizons/layers 
4. a horizon is the most granular unit 

For instance, our reference soil region Napa County, California (lkey=14083) comprises 146 map units, 500 components, and 714 horizons. 

Our unit of analysis is at the soil area level, but since most of the soil attributes (e.g. pH, clay percent) are provided at the horizon and component level, we need to aggregate these attributes up the soil hierarchy, from: 
* horizon/layer -> component (weighting by thickness of horizon/layer)
* component -> map unit (weighting by component percent, provided directly by NCRS)
* map unit -> area/legend (weighting by number of acres)

| Soil Hierarchy Level | HDFS File Path | Primary Key | Number of Rows | 
| ----------- | ----------- | ----------- | ----------- |
| Soil Area | /user/yjn214/rbda-proj/legend.txt | lkey | 3266 | 
| Map Unit | /user/yjn214/rbda-proj/mapunit.txt |  mukey | 320288 | 
| Component | /user/yjn214/rbda-proj/component.txt | cokey | 1188424 | 
| Horizon | /user/yjn214/rbda-proj/chorizon.txt | chkey | 3748010 | 

Data schemas may be found [here](ingest_data/Table%20schemas%20for%20soil%20database.pdf). 

## HDFS Output Data

Below are the output tables in HDFS and Hive that are of interest: 

| Table Content | HDFS File Path | Hive Table | 
| ----------- | ----------- | ----------- | 
| Soil Area Features | hdfs://dumbo/user/yjn214/rbda-proj/soil_features_local | yjn214.db/soil_features | 
| Soil Area Cosine Similarities | hdfs://dumbo/user/yjn214/rbda-proj/soil_cos_sim | yjn214.db/soil_sim |  


(solar README)
# Solar Irradiance Data
This folder contains the processing tools and select results of the NREL solar irradiance data used in the *Finding Napa* project.


## Folders
* `dataacquisition` contains the scripts for fetching NREL NSRDB data and output
* `rawprofileresults` contains the results of running the profiler scripts on the raw (unmodified) NREL solar irradiance data.
* `dataprocess` contains the tools used to featurize, standardize, and produce the
output cosine similarities used for the combination codes.

## Data Files
* `solarRegionsV2.csv` lists the solar regions and year for which data was obtained. Provides a translation between the soil regions (SoilLat, SoilLon) and the solar regions (SolarLat, SolarLon), as well as the soil and solar region keys.


## HDFS Data
* Full Solar Data Set: `/user/cjg507/solarData.csv` 
This data set *includes* the following schema:

| Column Name  | Data Type | Description | Range |
| ----------- | ----------- | ----------- | ----------- |
| Year      | Int       |  Year Represented | 2017 |
| Month      | Int       |  Month Represented | 1-12 |
| Day      | Int       |  Day Represented | 1-31 |
| Hour      | Int       |  Hour Represented | 0-23  |
| Minute      | Int       |  Minute Represented | 30 (hour midpoint) |
| GHI      | Int       |  Total amount of direct and diffuse solar radiation (Whr/m^2)  received on a horizontal surface during the 60-minute period ending at the timestamp | Min: 0, Max: 1137 |
| DHI      | Int       |  Amount of solar radiation (Whr/m^2) received from the sky (excluding the solar disk) on a horizontal surface during the 60-minute period ending at the timestamp | Min: 0, Max: 682 |
| DNI      | Int       |  Amount of solar radiation (Whr/m^2) received in a collimated beam on a surface normal to the sun during the 60-minute period ending at the timestamp | Min: 0, Max: 1121 |
| Latitude     | Float       |  Latitude of modeled data point | Min: -14.27, Max: 59.77 |
| Longitude     | Float       |  Longitude of modeled data point | Min: -170.38, Max: -64.82 |
**Note:** This schema does not reflect all columns within `solarData.csv`, but rather a subset that are considered useful for analysis. A full collection of column names is included in `Solar/solarSchema.csv`. 



* Sample of Solar Data Set: `/user/cjg507/solarDataSample.csv`

(weather README)
## Weather Data and Processing

The main dataset is hosted by NOAA at https://www.ncdc.noaa.gov/isd and can be download via FTP. The total disk space after decompression is approximately 270GB.

A sample line in the raw file looks like this:
2019 06 29 21   227    65 10180   310    15     0 -9999 -9999 ./720997-99999-2019

The fields are:
- Year
- Month
- Day
- Hour
- Temperature (Celsius x 10)
- Dew Point Temp (Celsius x 10)
- Sea Level Pressur (Hectopascals x 10)
- Wind Direction (Angle measured in a clockwise direction from true north)
- Wind Speed (meters / second)
- Sky Condition Total Coverage (the higher the number the more the overcast)
- Liquid Precipitation Depth (1hr)
- Liquid Precipitation Depth (6hr)
- Station Identifier and Year (see station_id.txt for station lat/lon)

## Folders

- src: map reduce program for featurizing Hive data that was generated from raw data
- src_norm: map reduce program for normalizing features, 1st pass puts everything int K-V pairs
- src_comb: map reduce program for combining K-V pairs back into original table
- src_cos: map reduce program for computing cosine similarity of normalized data

## Processing Overview

1. PIG program to convert raw data into a readable Hive format:
- Reading fix position format files is hard in Hive, so we used some pig scripts to transform the raw file into a comma delimited one. The script can be fund at the top of beeline.txt

2. Hive loading data:
- Section 2 of beeline.txt, loads the above PIG output into a Hive table

3. Hive aggregation data extraction:
- Section 3 of beeline.txt, using SQL, we extract our raw features by decade, region, month

4. Featurising extracted features:
- Combines the raw features that is 1-row-per-month into a wider table that has each month as a feature instead of adding a new row for it. Section 4 of beeline.txt.
- The code used is in the src/ folder

5. Normalizing features:
- Each feature is then normalized by subtracting mean and dividing by standard dev. This is accomplished by a 2-step Map Reduce program. 
- The first step explodes the data into KV pairs with the raw numerical data normalized, the code used is in the src_norm/ folder
- The 2nd step combines the KV pairs into the original matrix, with all numerical data points now normalized, the code used is in the src_comb/ folder

6. Computing Cosine Similarity:
- Napa's feature vector was hardcoded into the .py file and all other regions are computed against Napa, resulting in a similarity score for each region, with Napa's score being 1.0, code is in the src_cos/ folder.

(tools README)
# Common Project Tools
This folder contains the common project team scripts for various Hadoop and NYU Dumbo cluster applications.


## Folders
* `profiler` contains tools for profiling big data in HDFS using Hadoop MapReduce
* `cosineSim` contains tools for calculating cosine similarity 
* `findEntry` tools for finding records in a Hadoop HDFS file
* `profiler` automated Hadoop HDFS file profiling tools
* `normalizer` normalizes (standardizes) a Hadoop HDFS file of numerical data
* `voronoiPlotting` creates a Voronoi diagram for the US using Geopandas


### HadoopTools.py
Contains classes that form Python wrappers around common Hadoop tools. The *runHadoop* class automates the dispatch of MapReduce jobs and provides various methods of file management.






